{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5668bce7-d162-4a14-bfac-481bb0b3e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Credit Default Risk - Traditional ML + Fairness-First Approach\n",
    "# Based on 1st place solution with fairness considerations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01236b7c-e8dc-4cc8-87db-f9eb99e29d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Libraries\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b042aa63-1ae9-48e5-a0dc-5f120029bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e82105-9c39-498f-b67c-8675c7f0ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability & Fairness\n",
    "import shap\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import DemographicParity, ExponentiatedGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdca6300-71c8-4156-acde-e1470457f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0f8c40-5df8-4b3d-adbe-e9db0b2393c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from xgboost.callback import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98165a09-5408-4df3-a685-f3e69104d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d39747d-2ad3-4686-941e-cf0059ab0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_test.csv\t\t    home-credit-default-risk.zip\n",
      "application_train.csv\t\t    installments_payments.csv\n",
      "bureau_balance.csv\t\t    POS_CASH_balance.csv\n",
      "bureau.csv\t\t\t    previous_application.csv\n",
      "credit_card_balance.csv\t\t    sample_submission.csv\n",
      "HomeCredit_columns_description.csv  xloan.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db82475a-01f4-49a1-aa27-dd773aecf128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Home Credit dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HOME CREDIT RISK ASSESSMENT DEMO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded training data: (307511, 122)\n",
      "INFO:__main__:Target distribution: {0: 282686, 1: 24825}\n",
      "INFO:__main__:Starting feature engineering...\n",
      "INFO:__main__:Feature engineering completed. Shape: (307511, 137)\n",
      "INFO:__main__:Preparing features for modeling...\n",
      "INFO:__main__:Features prepared. Shape: (307511, 135)\n",
      "INFO:__main__:Selecting top 50 features...\n",
      "INFO:__main__:Selected 50 features\n",
      "INFO:__main__:Training models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Warning] verbosity is set=0, verbose=-1 will be ignored. Current value: verbosity=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] verbosity is set=0, verbose=-1 will be ignored. Current value: verbosity=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Warning] verbosity is set=0, verbose=-1 will be ignored. Current value: verbosity=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[0]\tvalidation_0-auc:0.72639\n",
      "[1]\tvalidation_0-auc:0.72980\n",
      "[2]\tvalidation_0-auc:0.73448\n",
      "[3]\tvalidation_0-auc:0.73641\n",
      "[4]\tvalidation_0-auc:0.73700\n",
      "[5]\tvalidation_0-auc:0.73815\n",
      "[6]\tvalidation_0-auc:0.73957\n",
      "[7]\tvalidation_0-auc:0.74032\n",
      "[8]\tvalidation_0-auc:0.74056\n",
      "[9]\tvalidation_0-auc:0.74041\n",
      "[10]\tvalidation_0-auc:0.74095\n",
      "[11]\tvalidation_0-auc:0.74154\n",
      "[12]\tvalidation_0-auc:0.74210\n",
      "[13]\tvalidation_0-auc:0.74252\n",
      "[14]\tvalidation_0-auc:0.74279\n",
      "[15]\tvalidation_0-auc:0.74344\n",
      "[16]\tvalidation_0-auc:0.74350\n",
      "[17]\tvalidation_0-auc:0.74405\n",
      "[18]\tvalidation_0-auc:0.74457\n",
      "[19]\tvalidation_0-auc:0.74473\n",
      "[20]\tvalidation_0-auc:0.74505\n",
      "[21]\tvalidation_0-auc:0.74538\n",
      "[22]\tvalidation_0-auc:0.74572\n",
      "[23]\tvalidation_0-auc:0.74602\n",
      "[24]\tvalidation_0-auc:0.74635\n",
      "[25]\tvalidation_0-auc:0.74681\n",
      "[26]\tvalidation_0-auc:0.74721\n",
      "[27]\tvalidation_0-auc:0.74749\n",
      "[28]\tvalidation_0-auc:0.74776\n",
      "[29]\tvalidation_0-auc:0.74798\n",
      "[30]\tvalidation_0-auc:0.74846\n",
      "[31]\tvalidation_0-auc:0.74880\n",
      "[32]\tvalidation_0-auc:0.74908\n",
      "[33]\tvalidation_0-auc:0.74943\n",
      "[34]\tvalidation_0-auc:0.74970\n",
      "[35]\tvalidation_0-auc:0.74993\n",
      "[36]\tvalidation_0-auc:0.75024\n",
      "[37]\tvalidation_0-auc:0.75041\n",
      "[38]\tvalidation_0-auc:0.75054\n",
      "[39]\tvalidation_0-auc:0.75093\n",
      "[40]\tvalidation_0-auc:0.75127\n",
      "[41]\tvalidation_0-auc:0.75161\n",
      "[42]\tvalidation_0-auc:0.75183\n",
      "[43]\tvalidation_0-auc:0.75202\n",
      "[44]\tvalidation_0-auc:0.75223\n",
      "[45]\tvalidation_0-auc:0.75231\n",
      "[46]\tvalidation_0-auc:0.75265\n",
      "[47]\tvalidation_0-auc:0.75280\n",
      "[48]\tvalidation_0-auc:0.75304\n",
      "[49]\tvalidation_0-auc:0.75323\n",
      "[50]\tvalidation_0-auc:0.75350\n",
      "[51]\tvalidation_0-auc:0.75369\n",
      "[52]\tvalidation_0-auc:0.75384\n",
      "[53]\tvalidation_0-auc:0.75396\n",
      "[54]\tvalidation_0-auc:0.75420\n",
      "[55]\tvalidation_0-auc:0.75435\n",
      "[56]\tvalidation_0-auc:0.75439\n",
      "[57]\tvalidation_0-auc:0.75467\n",
      "[58]\tvalidation_0-auc:0.75492\n",
      "[59]\tvalidation_0-auc:0.75511\n",
      "[60]\tvalidation_0-auc:0.75533\n",
      "[61]\tvalidation_0-auc:0.75552\n",
      "[62]\tvalidation_0-auc:0.75578\n",
      "[63]\tvalidation_0-auc:0.75591\n",
      "[64]\tvalidation_0-auc:0.75605\n",
      "[65]\tvalidation_0-auc:0.75619\n",
      "[66]\tvalidation_0-auc:0.75657\n",
      "[67]\tvalidation_0-auc:0.75672\n",
      "[68]\tvalidation_0-auc:0.75678\n",
      "[69]\tvalidation_0-auc:0.75686\n",
      "[70]\tvalidation_0-auc:0.75701\n",
      "[71]\tvalidation_0-auc:0.75706\n",
      "[72]\tvalidation_0-auc:0.75713\n",
      "[73]\tvalidation_0-auc:0.75740\n",
      "[74]\tvalidation_0-auc:0.75761\n",
      "[75]\tvalidation_0-auc:0.75771\n",
      "[76]\tvalidation_0-auc:0.75779\n",
      "[77]\tvalidation_0-auc:0.75793\n",
      "[78]\tvalidation_0-auc:0.75819\n",
      "[79]\tvalidation_0-auc:0.75823\n",
      "[80]\tvalidation_0-auc:0.75844\n",
      "[81]\tvalidation_0-auc:0.75866\n",
      "[82]\tvalidation_0-auc:0.75880\n",
      "[83]\tvalidation_0-auc:0.75893\n",
      "[84]\tvalidation_0-auc:0.75899\n",
      "[85]\tvalidation_0-auc:0.75904\n",
      "[86]\tvalidation_0-auc:0.75920\n",
      "[87]\tvalidation_0-auc:0.75922\n",
      "[88]\tvalidation_0-auc:0.75935\n",
      "[89]\tvalidation_0-auc:0.75949\n",
      "[90]\tvalidation_0-auc:0.75965\n",
      "[91]\tvalidation_0-auc:0.75993\n",
      "[92]\tvalidation_0-auc:0.76000\n",
      "[93]\tvalidation_0-auc:0.76009\n",
      "[94]\tvalidation_0-auc:0.76017\n",
      "[95]\tvalidation_0-auc:0.76025\n",
      "[96]\tvalidation_0-auc:0.76034\n",
      "[97]\tvalidation_0-auc:0.76042\n",
      "[98]\tvalidation_0-auc:0.76051\n",
      "[99]\tvalidation_0-auc:0.76059\n",
      "[100]\tvalidation_0-auc:0.76073\n",
      "[101]\tvalidation_0-auc:0.76090\n",
      "[102]\tvalidation_0-auc:0.76093\n",
      "[103]\tvalidation_0-auc:0.76096\n",
      "[104]\tvalidation_0-auc:0.76100\n",
      "[105]\tvalidation_0-auc:0.76100\n",
      "[106]\tvalidation_0-auc:0.76115\n",
      "[107]\tvalidation_0-auc:0.76124\n",
      "[108]\tvalidation_0-auc:0.76126\n",
      "[109]\tvalidation_0-auc:0.76133\n",
      "[110]\tvalidation_0-auc:0.76139\n",
      "[111]\tvalidation_0-auc:0.76152\n",
      "[112]\tvalidation_0-auc:0.76158\n",
      "[113]\tvalidation_0-auc:0.76168\n",
      "[114]\tvalidation_0-auc:0.76192\n",
      "[115]\tvalidation_0-auc:0.76193\n",
      "[116]\tvalidation_0-auc:0.76196\n",
      "[117]\tvalidation_0-auc:0.76198\n",
      "[118]\tvalidation_0-auc:0.76203\n",
      "[119]\tvalidation_0-auc:0.76208\n",
      "[120]\tvalidation_0-auc:0.76224\n",
      "[121]\tvalidation_0-auc:0.76242\n",
      "[122]\tvalidation_0-auc:0.76246\n",
      "[123]\tvalidation_0-auc:0.76255\n",
      "[124]\tvalidation_0-auc:0.76260\n",
      "[125]\tvalidation_0-auc:0.76266\n",
      "[126]\tvalidation_0-auc:0.76272\n",
      "[127]\tvalidation_0-auc:0.76288\n",
      "[128]\tvalidation_0-auc:0.76291\n",
      "[129]\tvalidation_0-auc:0.76291\n",
      "[130]\tvalidation_0-auc:0.76292\n",
      "[131]\tvalidation_0-auc:0.76295\n",
      "[132]\tvalidation_0-auc:0.76311\n",
      "[133]\tvalidation_0-auc:0.76312\n",
      "[134]\tvalidation_0-auc:0.76326\n",
      "[135]\tvalidation_0-auc:0.76331\n",
      "[136]\tvalidation_0-auc:0.76347\n",
      "[137]\tvalidation_0-auc:0.76350\n",
      "[138]\tvalidation_0-auc:0.76355\n",
      "[139]\tvalidation_0-auc:0.76369\n",
      "[140]\tvalidation_0-auc:0.76373\n",
      "[141]\tvalidation_0-auc:0.76378\n",
      "[142]\tvalidation_0-auc:0.76377\n",
      "[143]\tvalidation_0-auc:0.76375\n",
      "[144]\tvalidation_0-auc:0.76384\n",
      "[145]\tvalidation_0-auc:0.76387\n",
      "[146]\tvalidation_0-auc:0.76384\n",
      "[147]\tvalidation_0-auc:0.76395\n",
      "[148]\tvalidation_0-auc:0.76392\n",
      "[149]\tvalidation_0-auc:0.76396\n",
      "[150]\tvalidation_0-auc:0.76399\n",
      "[151]\tvalidation_0-auc:0.76401\n",
      "[152]\tvalidation_0-auc:0.76403\n",
      "[153]\tvalidation_0-auc:0.76405\n",
      "[154]\tvalidation_0-auc:0.76404\n",
      "[155]\tvalidation_0-auc:0.76411\n",
      "[156]\tvalidation_0-auc:0.76417\n",
      "[157]\tvalidation_0-auc:0.76421\n",
      "[158]\tvalidation_0-auc:0.76428\n",
      "[159]\tvalidation_0-auc:0.76436\n",
      "[160]\tvalidation_0-auc:0.76440\n",
      "[161]\tvalidation_0-auc:0.76440\n",
      "[162]\tvalidation_0-auc:0.76445\n",
      "[163]\tvalidation_0-auc:0.76449\n",
      "[164]\tvalidation_0-auc:0.76445\n",
      "[165]\tvalidation_0-auc:0.76449\n",
      "[166]\tvalidation_0-auc:0.76467\n",
      "[167]\tvalidation_0-auc:0.76477\n",
      "[168]\tvalidation_0-auc:0.76479\n",
      "[169]\tvalidation_0-auc:0.76483\n",
      "[170]\tvalidation_0-auc:0.76489\n",
      "[171]\tvalidation_0-auc:0.76492\n",
      "[172]\tvalidation_0-auc:0.76504\n",
      "[173]\tvalidation_0-auc:0.76504\n",
      "[174]\tvalidation_0-auc:0.76509\n",
      "[175]\tvalidation_0-auc:0.76515\n",
      "[176]\tvalidation_0-auc:0.76520\n",
      "[177]\tvalidation_0-auc:0.76519\n",
      "[178]\tvalidation_0-auc:0.76521\n",
      "[179]\tvalidation_0-auc:0.76523\n",
      "[180]\tvalidation_0-auc:0.76528\n",
      "[181]\tvalidation_0-auc:0.76531\n",
      "[182]\tvalidation_0-auc:0.76535\n",
      "[183]\tvalidation_0-auc:0.76538\n",
      "[184]\tvalidation_0-auc:0.76538\n",
      "[185]\tvalidation_0-auc:0.76534\n",
      "[186]\tvalidation_0-auc:0.76537\n",
      "[187]\tvalidation_0-auc:0.76544\n",
      "[188]\tvalidation_0-auc:0.76548\n",
      "[189]\tvalidation_0-auc:0.76556\n",
      "[190]\tvalidation_0-auc:0.76557\n",
      "[191]\tvalidation_0-auc:0.76560\n",
      "[192]\tvalidation_0-auc:0.76564\n",
      "[193]\tvalidation_0-auc:0.76565\n",
      "[194]\tvalidation_0-auc:0.76571\n",
      "[195]\tvalidation_0-auc:0.76572\n",
      "[196]\tvalidation_0-auc:0.76570\n",
      "[197]\tvalidation_0-auc:0.76579\n",
      "[198]\tvalidation_0-auc:0.76575\n",
      "[199]\tvalidation_0-auc:0.76571\n",
      "[200]\tvalidation_0-auc:0.76565\n",
      "[201]\tvalidation_0-auc:0.76564\n",
      "[202]\tvalidation_0-auc:0.76566\n",
      "[203]\tvalidation_0-auc:0.76572\n",
      "[204]\tvalidation_0-auc:0.76573\n",
      "[205]\tvalidation_0-auc:0.76571\n",
      "[206]\tvalidation_0-auc:0.76577\n",
      "[207]\tvalidation_0-auc:0.76577\n",
      "[208]\tvalidation_0-auc:0.76579\n",
      "[209]\tvalidation_0-auc:0.76582\n",
      "[210]\tvalidation_0-auc:0.76587\n",
      "[211]\tvalidation_0-auc:0.76591\n",
      "[212]\tvalidation_0-auc:0.76594\n",
      "[213]\tvalidation_0-auc:0.76604\n",
      "[214]\tvalidation_0-auc:0.76607\n",
      "[215]\tvalidation_0-auc:0.76610\n",
      "[216]\tvalidation_0-auc:0.76607\n",
      "[217]\tvalidation_0-auc:0.76608\n",
      "[218]\tvalidation_0-auc:0.76609\n",
      "[219]\tvalidation_0-auc:0.76614\n",
      "[220]\tvalidation_0-auc:0.76615\n",
      "[221]\tvalidation_0-auc:0.76621\n",
      "[222]\tvalidation_0-auc:0.76625\n",
      "[223]\tvalidation_0-auc:0.76625\n",
      "[224]\tvalidation_0-auc:0.76634\n",
      "[225]\tvalidation_0-auc:0.76634\n",
      "[226]\tvalidation_0-auc:0.76635\n",
      "[227]\tvalidation_0-auc:0.76641\n",
      "[228]\tvalidation_0-auc:0.76640\n",
      "[229]\tvalidation_0-auc:0.76639\n",
      "[230]\tvalidation_0-auc:0.76638\n",
      "[231]\tvalidation_0-auc:0.76646\n",
      "[232]\tvalidation_0-auc:0.76651\n",
      "[233]\tvalidation_0-auc:0.76655\n",
      "[234]\tvalidation_0-auc:0.76653\n",
      "[235]\tvalidation_0-auc:0.76654\n",
      "[236]\tvalidation_0-auc:0.76653\n",
      "[237]\tvalidation_0-auc:0.76652\n",
      "[238]\tvalidation_0-auc:0.76654\n",
      "[239]\tvalidation_0-auc:0.76654\n",
      "[240]\tvalidation_0-auc:0.76658\n",
      "[241]\tvalidation_0-auc:0.76657\n",
      "[242]\tvalidation_0-auc:0.76659\n",
      "[243]\tvalidation_0-auc:0.76658\n",
      "[244]\tvalidation_0-auc:0.76657\n",
      "[245]\tvalidation_0-auc:0.76664\n",
      "[246]\tvalidation_0-auc:0.76661\n",
      "[247]\tvalidation_0-auc:0.76665\n",
      "[248]\tvalidation_0-auc:0.76666\n",
      "[249]\tvalidation_0-auc:0.76663\n",
      "[250]\tvalidation_0-auc:0.76667\n",
      "[251]\tvalidation_0-auc:0.76666\n",
      "[252]\tvalidation_0-auc:0.76671\n",
      "[253]\tvalidation_0-auc:0.76675\n",
      "[254]\tvalidation_0-auc:0.76675\n",
      "[255]\tvalidation_0-auc:0.76672\n",
      "[256]\tvalidation_0-auc:0.76674\n",
      "[257]\tvalidation_0-auc:0.76681\n",
      "[258]\tvalidation_0-auc:0.76683\n",
      "[259]\tvalidation_0-auc:0.76681\n",
      "[260]\tvalidation_0-auc:0.76684\n",
      "[261]\tvalidation_0-auc:0.76682\n",
      "[262]\tvalidation_0-auc:0.76684\n",
      "[263]\tvalidation_0-auc:0.76682\n",
      "[264]\tvalidation_0-auc:0.76683\n",
      "[265]\tvalidation_0-auc:0.76689\n",
      "[266]\tvalidation_0-auc:0.76690\n",
      "[267]\tvalidation_0-auc:0.76689\n",
      "[268]\tvalidation_0-auc:0.76689\n",
      "[269]\tvalidation_0-auc:0.76689\n",
      "[270]\tvalidation_0-auc:0.76689\n",
      "[271]\tvalidation_0-auc:0.76691\n",
      "[272]\tvalidation_0-auc:0.76691\n",
      "[273]\tvalidation_0-auc:0.76690\n",
      "[274]\tvalidation_0-auc:0.76696\n",
      "[275]\tvalidation_0-auc:0.76697\n",
      "[276]\tvalidation_0-auc:0.76699\n",
      "[277]\tvalidation_0-auc:0.76707\n",
      "[278]\tvalidation_0-auc:0.76708\n",
      "[279]\tvalidation_0-auc:0.76710\n",
      "[280]\tvalidation_0-auc:0.76711\n",
      "[281]\tvalidation_0-auc:0.76710\n",
      "[282]\tvalidation_0-auc:0.76712\n",
      "[283]\tvalidation_0-auc:0.76713\n",
      "[284]\tvalidation_0-auc:0.76713\n",
      "[285]\tvalidation_0-auc:0.76715\n",
      "[286]\tvalidation_0-auc:0.76714\n",
      "[287]\tvalidation_0-auc:0.76716\n",
      "[288]\tvalidation_0-auc:0.76720\n",
      "[289]\tvalidation_0-auc:0.76720\n",
      "[290]\tvalidation_0-auc:0.76724\n",
      "[291]\tvalidation_0-auc:0.76722\n",
      "[292]\tvalidation_0-auc:0.76722\n",
      "[293]\tvalidation_0-auc:0.76722\n",
      "[294]\tvalidation_0-auc:0.76722\n",
      "[295]\tvalidation_0-auc:0.76725\n",
      "[296]\tvalidation_0-auc:0.76730\n",
      "[297]\tvalidation_0-auc:0.76733\n",
      "[298]\tvalidation_0-auc:0.76730\n",
      "[299]\tvalidation_0-auc:0.76735\n",
      "[300]\tvalidation_0-auc:0.76731\n",
      "[301]\tvalidation_0-auc:0.76730\n",
      "[302]\tvalidation_0-auc:0.76740\n",
      "[303]\tvalidation_0-auc:0.76740\n",
      "[304]\tvalidation_0-auc:0.76741\n",
      "[305]\tvalidation_0-auc:0.76741\n",
      "[306]\tvalidation_0-auc:0.76740\n",
      "[307]\tvalidation_0-auc:0.76741\n",
      "[308]\tvalidation_0-auc:0.76742\n",
      "[309]\tvalidation_0-auc:0.76742\n",
      "[310]\tvalidation_0-auc:0.76745\n",
      "[311]\tvalidation_0-auc:0.76744\n",
      "[312]\tvalidation_0-auc:0.76746\n",
      "[313]\tvalidation_0-auc:0.76747\n",
      "[314]\tvalidation_0-auc:0.76744\n",
      "[315]\tvalidation_0-auc:0.76746\n",
      "[316]\tvalidation_0-auc:0.76749\n",
      "[317]\tvalidation_0-auc:0.76747\n",
      "[318]\tvalidation_0-auc:0.76743\n",
      "[319]\tvalidation_0-auc:0.76749\n",
      "[320]\tvalidation_0-auc:0.76752\n",
      "[321]\tvalidation_0-auc:0.76754\n",
      "[322]\tvalidation_0-auc:0.76757\n",
      "[323]\tvalidation_0-auc:0.76754\n",
      "[324]\tvalidation_0-auc:0.76750\n",
      "[325]\tvalidation_0-auc:0.76751\n",
      "[326]\tvalidation_0-auc:0.76747\n",
      "[327]\tvalidation_0-auc:0.76746\n",
      "[328]\tvalidation_0-auc:0.76752\n",
      "[329]\tvalidation_0-auc:0.76754\n",
      "[330]\tvalidation_0-auc:0.76762\n",
      "[331]\tvalidation_0-auc:0.76763\n",
      "[332]\tvalidation_0-auc:0.76759\n",
      "[333]\tvalidation_0-auc:0.76759\n",
      "[334]\tvalidation_0-auc:0.76763\n",
      "[335]\tvalidation_0-auc:0.76760\n",
      "[336]\tvalidation_0-auc:0.76760\n",
      "[337]\tvalidation_0-auc:0.76761\n",
      "[338]\tvalidation_0-auc:0.76758\n",
      "[339]\tvalidation_0-auc:0.76759\n",
      "[340]\tvalidation_0-auc:0.76758\n",
      "[341]\tvalidation_0-auc:0.76758\n",
      "[342]\tvalidation_0-auc:0.76763\n",
      "[343]\tvalidation_0-auc:0.76761\n",
      "[344]\tvalidation_0-auc:0.76757\n",
      "[345]\tvalidation_0-auc:0.76756\n",
      "[346]\tvalidation_0-auc:0.76758\n",
      "[347]\tvalidation_0-auc:0.76752\n",
      "[348]\tvalidation_0-auc:0.76754\n",
      "[349]\tvalidation_0-auc:0.76757\n",
      "[350]\tvalidation_0-auc:0.76755\n",
      "[351]\tvalidation_0-auc:0.76756\n",
      "[352]\tvalidation_0-auc:0.76763\n",
      "[353]\tvalidation_0-auc:0.76757\n",
      "[354]\tvalidation_0-auc:0.76758\n",
      "[355]\tvalidation_0-auc:0.76753\n",
      "[356]\tvalidation_0-auc:0.76752\n",
      "[357]\tvalidation_0-auc:0.76753\n",
      "[358]\tvalidation_0-auc:0.76755\n",
      "[359]\tvalidation_0-auc:0.76753\n",
      "[360]\tvalidation_0-auc:0.76753\n",
      "[361]\tvalidation_0-auc:0.76753\n",
      "[362]\tvalidation_0-auc:0.76753\n",
      "[363]\tvalidation_0-auc:0.76756\n",
      "[364]\tvalidation_0-auc:0.76754\n",
      "[365]\tvalidation_0-auc:0.76756\n",
      "[366]\tvalidation_0-auc:0.76758\n",
      "[367]\tvalidation_0-auc:0.76756\n",
      "[368]\tvalidation_0-auc:0.76753\n",
      "[369]\tvalidation_0-auc:0.76752\n",
      "[370]\tvalidation_0-auc:0.76753\n",
      "[371]\tvalidation_0-auc:0.76752\n",
      "[372]\tvalidation_0-auc:0.76751\n",
      "[373]\tvalidation_0-auc:0.76754\n",
      "[374]\tvalidation_0-auc:0.76751\n",
      "[375]\tvalidation_0-auc:0.76750\n",
      "[376]\tvalidation_0-auc:0.76749\n",
      "[377]\tvalidation_0-auc:0.76751\n",
      "[378]\tvalidation_0-auc:0.76753\n",
      "[379]\tvalidation_0-auc:0.76755\n",
      "[380]\tvalidation_0-auc:0.76752\n",
      "[381]\tvalidation_0-auc:0.76754\n",
      "[382]\tvalidation_0-auc:0.76751\n",
      "[383]\tvalidation_0-auc:0.76750\n",
      "[384]\tvalidation_0-auc:0.76750\n",
      "[385]\tvalidation_0-auc:0.76749\n",
      "[386]\tvalidation_0-auc:0.76751\n",
      "[387]\tvalidation_0-auc:0.76754\n",
      "[388]\tvalidation_0-auc:0.76755\n",
      "[389]\tvalidation_0-auc:0.76756\n",
      "[390]\tvalidation_0-auc:0.76754\n",
      "[391]\tvalidation_0-auc:0.76755\n",
      "[392]\tvalidation_0-auc:0.76753\n",
      "[393]\tvalidation_0-auc:0.76751\n",
      "[394]\tvalidation_0-auc:0.76748\n",
      "[395]\tvalidation_0-auc:0.76751\n",
      "[396]\tvalidation_0-auc:0.76754\n",
      "[397]\tvalidation_0-auc:0.76755\n",
      "[398]\tvalidation_0-auc:0.76751\n",
      "[399]\tvalidation_0-auc:0.76751\n",
      "[400]\tvalidation_0-auc:0.76752\n",
      "[401]\tvalidation_0-auc:0.76753\n",
      "[402]\tvalidation_0-auc:0.76754\n",
      "[403]\tvalidation_0-auc:0.76750\n",
      "[404]\tvalidation_0-auc:0.76752\n",
      "[405]\tvalidation_0-auc:0.76751\n",
      "[406]\tvalidation_0-auc:0.76753\n",
      "[407]\tvalidation_0-auc:0.76751\n",
      "[408]\tvalidation_0-auc:0.76751\n",
      "[409]\tvalidation_0-auc:0.76749\n",
      "[410]\tvalidation_0-auc:0.76748\n",
      "[411]\tvalidation_0-auc:0.76746\n",
      "[412]\tvalidation_0-auc:0.76749\n",
      "[413]\tvalidation_0-auc:0.76749\n",
      "[414]\tvalidation_0-auc:0.76751\n",
      "[415]\tvalidation_0-auc:0.76753\n",
      "[416]\tvalidation_0-auc:0.76755\n",
      "[417]\tvalidation_0-auc:0.76754\n",
      "[418]\tvalidation_0-auc:0.76758\n",
      "[419]\tvalidation_0-auc:0.76762\n",
      "[420]\tvalidation_0-auc:0.76765\n",
      "[421]\tvalidation_0-auc:0.76766\n",
      "[422]\tvalidation_0-auc:0.76767\n",
      "[423]\tvalidation_0-auc:0.76767\n",
      "[424]\tvalidation_0-auc:0.76773\n",
      "[425]\tvalidation_0-auc:0.76768\n",
      "[426]\tvalidation_0-auc:0.76768\n",
      "[427]\tvalidation_0-auc:0.76764\n",
      "[428]\tvalidation_0-auc:0.76767\n",
      "[429]\tvalidation_0-auc:0.76766\n",
      "[430]\tvalidation_0-auc:0.76766\n",
      "[431]\tvalidation_0-auc:0.76766\n",
      "[432]\tvalidation_0-auc:0.76761\n",
      "[433]\tvalidation_0-auc:0.76764\n",
      "[434]\tvalidation_0-auc:0.76768\n",
      "[435]\tvalidation_0-auc:0.76774\n",
      "[436]\tvalidation_0-auc:0.76772\n",
      "[437]\tvalidation_0-auc:0.76769\n",
      "[438]\tvalidation_0-auc:0.76768\n",
      "[439]\tvalidation_0-auc:0.76766\n",
      "[440]\tvalidation_0-auc:0.76766\n",
      "[441]\tvalidation_0-auc:0.76764\n",
      "[442]\tvalidation_0-auc:0.76766\n",
      "[443]\tvalidation_0-auc:0.76763\n",
      "[444]\tvalidation_0-auc:0.76766\n",
      "[445]\tvalidation_0-auc:0.76766\n",
      "[446]\tvalidation_0-auc:0.76764\n",
      "[447]\tvalidation_0-auc:0.76763\n",
      "[448]\tvalidation_0-auc:0.76765\n",
      "[449]\tvalidation_0-auc:0.76764\n",
      "[450]\tvalidation_0-auc:0.76764\n",
      "[451]\tvalidation_0-auc:0.76764\n",
      "[452]\tvalidation_0-auc:0.76761\n",
      "[453]\tvalidation_0-auc:0.76756\n",
      "[454]\tvalidation_0-auc:0.76756\n",
      "[455]\tvalidation_0-auc:0.76756\n",
      "[456]\tvalidation_0-auc:0.76758\n",
      "[457]\tvalidation_0-auc:0.76759\n",
      "[458]\tvalidation_0-auc:0.76757\n",
      "[459]\tvalidation_0-auc:0.76756\n",
      "[460]\tvalidation_0-auc:0.76759\n",
      "[461]\tvalidation_0-auc:0.76763\n",
      "[462]\tvalidation_0-auc:0.76759\n",
      "[463]\tvalidation_0-auc:0.76761\n",
      "[464]\tvalidation_0-auc:0.76764\n",
      "[465]\tvalidation_0-auc:0.76767\n",
      "[466]\tvalidation_0-auc:0.76762\n",
      "[467]\tvalidation_0-auc:0.76763\n",
      "[468]\tvalidation_0-auc:0.76765\n",
      "[469]\tvalidation_0-auc:0.76762\n",
      "[470]\tvalidation_0-auc:0.76761\n",
      "[471]\tvalidation_0-auc:0.76757\n",
      "[472]\tvalidation_0-auc:0.76760\n",
      "[473]\tvalidation_0-auc:0.76762\n",
      "[474]\tvalidation_0-auc:0.76762\n",
      "[475]\tvalidation_0-auc:0.76762\n",
      "[476]\tvalidation_0-auc:0.76761\n",
      "[477]\tvalidation_0-auc:0.76759\n",
      "[478]\tvalidation_0-auc:0.76759\n",
      "[479]\tvalidation_0-auc:0.76761\n",
      "[480]\tvalidation_0-auc:0.76759\n",
      "[481]\tvalidation_0-auc:0.76757\n",
      "[482]\tvalidation_0-auc:0.76766\n",
      "[483]\tvalidation_0-auc:0.76765\n",
      "[484]\tvalidation_0-auc:0.76767\n",
      "[485]\tvalidation_0-auc:0.76771\n",
      "[486]\tvalidation_0-auc:0.76770\n",
      "[487]\tvalidation_0-auc:0.76771\n",
      "[488]\tvalidation_0-auc:0.76770\n",
      "[489]\tvalidation_0-auc:0.76766\n",
      "[490]\tvalidation_0-auc:0.76765\n",
      "[491]\tvalidation_0-auc:0.76762\n",
      "[492]\tvalidation_0-auc:0.76760\n",
      "[493]\tvalidation_0-auc:0.76754\n",
      "[494]\tvalidation_0-auc:0.76750\n",
      "[495]\tvalidation_0-auc:0.76742\n",
      "[496]\tvalidation_0-auc:0.76740\n",
      "[497]\tvalidation_0-auc:0.76739\n",
      "[498]\tvalidation_0-auc:0.76737\n",
      "[499]\tvalidation_0-auc:0.76739\n",
      "[500]\tvalidation_0-auc:0.76735\n",
      "[501]\tvalidation_0-auc:0.76736\n",
      "[502]\tvalidation_0-auc:0.76736\n",
      "[503]\tvalidation_0-auc:0.76736\n",
      "[504]\tvalidation_0-auc:0.76736\n",
      "[505]\tvalidation_0-auc:0.76736\n",
      "[506]\tvalidation_0-auc:0.76734\n",
      "[507]\tvalidation_0-auc:0.76729\n",
      "[508]\tvalidation_0-auc:0.76731\n",
      "[509]\tvalidation_0-auc:0.76730\n",
      "[510]\tvalidation_0-auc:0.76729\n",
      "[511]\tvalidation_0-auc:0.76724\n",
      "[512]\tvalidation_0-auc:0.76723\n",
      "[513]\tvalidation_0-auc:0.76719\n",
      "[514]\tvalidation_0-auc:0.76716\n",
      "[515]\tvalidation_0-auc:0.76712\n",
      "[516]\tvalidation_0-auc:0.76710\n",
      "[517]\tvalidation_0-auc:0.76710\n",
      "[518]\tvalidation_0-auc:0.76710\n",
      "[519]\tvalidation_0-auc:0.76716\n",
      "[520]\tvalidation_0-auc:0.76712\n",
      "[521]\tvalidation_0-auc:0.76711\n",
      "[522]\tvalidation_0-auc:0.76710\n",
      "[523]\tvalidation_0-auc:0.76710\n",
      "[524]\tvalidation_0-auc:0.76713\n",
      "[525]\tvalidation_0-auc:0.76712\n",
      "[526]\tvalidation_0-auc:0.76710\n",
      "[527]\tvalidation_0-auc:0.76708\n",
      "[528]\tvalidation_0-auc:0.76706\n",
      "[529]\tvalidation_0-auc:0.76702\n",
      "[530]\tvalidation_0-auc:0.76709\n",
      "[531]\tvalidation_0-auc:0.76712\n",
      "[532]\tvalidation_0-auc:0.76712\n",
      "[533]\tvalidation_0-auc:0.76708\n",
      "[534]\tvalidation_0-auc:0.76705\n",
      "[535]\tvalidation_0-auc:0.76705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Models trained successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] verbosity is set=0, verbose=-1 will be ignored. Current value: verbosity=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluating fairness metrics...\n",
      "INFO:__main__:gender - DP diff: 0.0042, EO diff: 0.0254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:age_group - DP diff: 0.0043, EO diff: 0.0235\n",
      "INFO:__main__:education - DP diff: 0.0038, EO diff: 0.0366\n",
      "INFO:__main__:Setting up SHAP explainer...\n",
      "INFO:__main__:SHAP explainer ready\n",
      "INFO:__main__:Model saved to home_credit_demo_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example explanation ready for customer 0\n",
      "Demo completed successfully!\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "✅ Traditional ML ensemble trained\n",
      "✅ Fairness metrics calculated\n",
      "✅ SHAP explanations ready\n",
      "✅ Model saved for production\n"
     ]
    }
   ],
   "source": [
    "class HomeCreditRiskAssessment:\n",
    "    \"\"\"\n",
    "    Home Credit Risk Assessment với Traditional ML + Fairness-First approach\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_importance = {}\n",
    "        self.fairness_metrics = {}\n",
    "        self.explainer = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_selector = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load và basic preprocessing của Home Credit dataset\"\"\"\n",
    "        logger.info(\"Loading Home Credit dataset...\")\n",
    "        \n",
    "        # Load main tables\n",
    "        try:\n",
    "            self.app_train = pd.read_csv('application_train.csv')\n",
    "            self.app_test = pd.read_csv('application_test.csv')\n",
    "            self.bureau = pd.read_csv('bureau.csv')\n",
    "            self.bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "            self.prev_app = pd.read_csv('previous_application.csv')\n",
    "            self.pos_cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "            self.installments = pd.read_csv('installments_payments.csv')\n",
    "            self.credit_card = pd.read_csv('credit_card_balance.csv')\n",
    "            \n",
    "            logger.info(f\"Loaded training data: {self.app_train.shape}\")\n",
    "            logger.info(f\"Target distribution: {self.app_train['TARGET'].value_counts().to_dict()}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"File not found: {e}\")\n",
    "            # Create dummy data for demo\n",
    "            self._create_dummy_data()\n",
    "    \n",
    "    def _create_dummy_data(self):\n",
    "        \"\"\"Tạo dummy data cho demo khi không có file gốc\"\"\"\n",
    "        logger.info(\"Creating dummy data for demo...\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        n_samples = 10000\n",
    "        \n",
    "        # Main application data\n",
    "        self.app_train = pd.DataFrame({\n",
    "            'SK_ID_CURR': range(n_samples),\n",
    "            'TARGET': np.random.choice([0, 1], n_samples, p=[0.92, 0.08]),\n",
    "            'AMT_INCOME_TOTAL': np.random.lognormal(12, 0.5, n_samples),\n",
    "            'AMT_CREDIT': np.random.lognormal(13, 0.6, n_samples),\n",
    "            'AMT_ANNUITY': np.random.lognormal(10, 0.4, n_samples),\n",
    "            'AMT_GOODS_PRICE': np.random.lognormal(12.5, 0.7, n_samples),\n",
    "            'DAYS_BIRTH': np.random.randint(-25000, -6000, n_samples),\n",
    "            'DAYS_EMPLOYED': np.random.randint(-15000, 0, n_samples),\n",
    "            'CODE_GENDER': np.random.choice(['M', 'F'], n_samples, p=[0.35, 0.65]),\n",
    "            'NAME_EDUCATION_TYPE': np.random.choice([\n",
    "                'Secondary / secondary special', 'Higher education', \n",
    "                'Incomplete higher', 'Lower secondary'\n",
    "            ], n_samples, p=[0.7, 0.2, 0.08, 0.02]),\n",
    "            'NAME_FAMILY_STATUS': np.random.choice([\n",
    "                'Married', 'Single / not married', 'Civil marriage', 'Separated', 'Widow'\n",
    "            ], n_samples, p=[0.6, 0.2, 0.1, 0.05, 0.05]),\n",
    "            'CNT_CHILDREN': np.random.poisson(0.4, n_samples),\n",
    "            'EXT_SOURCE_1': np.random.beta(2, 3, n_samples),\n",
    "            'EXT_SOURCE_2': np.random.beta(2, 3, n_samples),\n",
    "            'EXT_SOURCE_3': np.random.beta(2, 3, n_samples),\n",
    "            'REGION_POPULATION_RELATIVE': np.random.uniform(0.0001, 0.1, n_samples)\n",
    "        })\n",
    "        \n",
    "        # Test data\n",
    "        self.app_test = self.app_train.copy()\n",
    "        self.app_test = self.app_test.drop('TARGET', axis=1)\n",
    "        self.app_test['SK_ID_CURR'] = range(n_samples, n_samples*2)\n",
    "        \n",
    "        logger.info(\"Dummy data created successfully\")\n",
    "    \n",
    "    def feature_engineering(self):\n",
    "        \"\"\"Feature engineering dựa trên winning solution\"\"\"\n",
    "        logger.info(\"Starting feature engineering...\")\n",
    "        \n",
    "        df = self.app_train.copy()\n",
    "        \n",
    "        # 1. Basic derived features\n",
    "        df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "        df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "        df['CREDIT_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "        df['GOODS_PRICE_CREDIT_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_CREDIT']\n",
    "        \n",
    "        # 2. Age and employment features\n",
    "        df['AGE_YEARS'] = (-df['DAYS_BIRTH'] / 365).astype(int)\n",
    "        df['EMPLOYED_YEARS'] = (-df['DAYS_EMPLOYED'] / 365).astype(int)\n",
    "        df['EMPLOYED_YEARS'] = df['EMPLOYED_YEARS'].apply(lambda x: max(0, x))\n",
    "        \n",
    "        # Age groups for fairness analysis\n",
    "        df['AGE_GROUP'] = pd.cut(df['AGE_YEARS'], \n",
    "                                bins=[0, 25, 35, 45, 55, 100], \n",
    "                                labels=['18-25', '26-35', '36-45', '46-55', '55+'])\n",
    "        \n",
    "        # 3. Income brackets\n",
    "        df['INCOME_BRACKET'] = pd.qcut(df['AMT_INCOME_TOTAL'], \n",
    "                                      q=5, \n",
    "                                      labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "        \n",
    "        # 4. External source combinations (từ winning solution)\n",
    "        df['EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "        df['EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "        df['EXT_SOURCE_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "        \n",
    "        # 5. Binary indicators\n",
    "        df['HAS_CAR'] = (df.get('FLAG_OWN_CAR', 'N') == 'Y').astype(int)\n",
    "        df['HAS_REALTY'] = (df.get('FLAG_OWN_REALTY', 'N') == 'Y').astype(int)\n",
    "        df['HAS_CHILDREN'] = (df['CNT_CHILDREN'] > 0).astype(int)\n",
    "        \n",
    "        # 6. Risk categories (business logic)\n",
    "        df['HIGH_CREDIT_RISK'] = (\n",
    "            (df['CREDIT_INCOME_RATIO'] > 10) | \n",
    "            (df['ANNUITY_INCOME_RATIO'] > 0.5)\n",
    "        ).astype(int)\n",
    "        \n",
    "        self.df_engineered = df\n",
    "        logger.info(f\"Feature engineering completed. Shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"Chuẩn bị features cho modeling\"\"\"\n",
    "        logger.info(\"Preparing features for modeling...\")\n",
    "        \n",
    "        df = self.df_engineered.copy()\n",
    "        \n",
    "        # Identify all categorical features dynamically\n",
    "        categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Label encoding for all categorical features\n",
    "        for col in categorical_features:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        # Features for modeling (loại bỏ ID và target)\n",
    "        exclude_cols = ['SK_ID_CURR', 'TARGET']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Handle missing values for numeric columns\n",
    "        numeric_feature_columns = df[feature_cols].select_dtypes(include=np.number).columns\n",
    "        medians = df[numeric_feature_columns].median()\n",
    "        df[numeric_feature_columns] = df[numeric_feature_columns].fillna(medians)\n",
    "        \n",
    "        self.X = df[feature_cols]\n",
    "        self.y = df['TARGET']\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        # Protected attributes cho fairness\n",
    "        self.protected_attrs = {\n",
    "            'gender': df['CODE_GENDER'],\n",
    "            'age_group': df['AGE_GROUP'],\n",
    "            'education': df['NAME_EDUCATION_TYPE']\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Features prepared. Shape: {self.X.shape}\")\n",
    "        return self.X, self.y\n",
    "    \n",
    "    def feature_selection(self, k=100):\n",
    "        \"\"\"Feature selection using statistical tests\"\"\"\n",
    "        logger.info(f\"Selecting top {k} features...\")\n",
    "        \n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_selected = self.feature_selector.fit_transform(self.X, self.y)\n",
    "        \n",
    "        # Get selected feature names\n",
    "        selected_mask = self.feature_selector.get_support()\n",
    "        self.selected_features = [self.feature_names[i] for i, selected in enumerate(selected_mask) if selected]\n",
    "        \n",
    "        logger.info(f\"Selected {len(self.selected_features)} features\")\n",
    "        return X_selected, self.selected_features\n",
    "    \n",
    "    def train_models(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Train ensemble của LightGBM, XGBoost, và Logistic Regression\"\"\"\n",
    "        logger.info(\"Training models...\")\n",
    "        \n",
    "        # 1. LightGBM\n",
    "        lgb_params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        self.models['lgb'] = lgb.LGBMClassifier(**lgb_params, n_estimators=1000, early_stopping_rounds=100, verbosity=0)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.models['lgb'].fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "            )\n",
    "        else:\n",
    "            self.models['lgb'].fit(X_train, y_train)\n",
    "        \n",
    "        # 2. XGBoost\n",
    "        xgb_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "        }\n",
    "        \n",
    "        self.models['xgb'] = xgb.XGBClassifier(**xgb_params, n_estimators=1000, early_stopping_rounds=100, verbosity=0)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.models['xgb'].fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "            )\n",
    "        else:\n",
    "            self.models['xgb'].fit(X_train, y_train)\n",
    "        \n",
    "        # 3. Logistic Regression (for interpretability)\n",
    "        self.models['lr'] = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        self.models['lr'].fit(X_train_scaled, y_train)\n",
    "        \n",
    "        logger.info(\"Models trained successfully\")\n",
    "        \n",
    "    def predict_ensemble(self, X):\n",
    "        \"\"\"Ensemble prediction với trọng số tối ưu\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # LightGBM prediction\n",
    "        predictions['lgb'] = self.models['lgb'].predict_proba(X)[:, 1]\n",
    "        \n",
    "        # XGBoost prediction  \n",
    "        predictions['xgb'] = self.models['xgb'].predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Logistic Regression prediction\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions['lr'] = self.models['lr'].predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Ensemble với trọng số từ winning solution\n",
    "        ensemble_pred = (0.6 * predictions['lgb'] + \n",
    "                        0.3 * predictions['xgb'] + \n",
    "                        0.1 * predictions['lr'])\n",
    "        \n",
    "        return ensemble_pred, predictions\n",
    "    \n",
    "    def evaluate_fairness(self, y_true, y_pred, protected_attr):\n",
    "        \"\"\"Đánh giá fairness metrics\"\"\"\n",
    "        logger.info(\"Evaluating fairness metrics...\")\n",
    "        \n",
    "        fairness_metrics = {}\n",
    "        \n",
    "        for attr_name, attr_values in protected_attr.items():\n",
    "            # Demographics parity difference\n",
    "            dp_diff = demographic_parity_difference(\n",
    "                y_true, y_pred > 0.5, sensitive_features=attr_values\n",
    "            )\n",
    "            \n",
    "            # Equalized odds difference\n",
    "            eo_diff = equalized_odds_difference(\n",
    "                y_true, y_pred > 0.5, sensitive_features=attr_values\n",
    "            )\n",
    "            \n",
    "            fairness_metrics[attr_name] = {\n",
    "                'demographic_parity_diff': dp_diff,\n",
    "                'equalized_odds_diff': eo_diff\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"{attr_name} - DP diff: {dp_diff:.4f}, EO diff: {eo_diff:.4f}\")\n",
    "        \n",
    "        self.fairness_metrics = fairness_metrics\n",
    "        return fairness_metrics\n",
    "    \n",
    "    def setup_explainability(self, X_sample):\n",
    "        \"\"\"Setup SHAP explainer\"\"\"\n",
    "        logger.info(\"Setting up SHAP explainer...\")\n",
    "        \n",
    "        # Use LightGBM for SHAP (fastest for tree models)\n",
    "        self.explainer = shap.TreeExplainer(self.models['lgb'])\n",
    "        self.shap_values = self.explainer.shap_values(X_sample) # This creates SHAP values for all classes in a multi-class problem.\n",
    "                                                               # For binary classification, shap_values[1] is typically used for the positive class.\n",
    "                                                               # Or, if shap_values is a list of two arrays (one for each class), then shap_values[1] is for class 1.\n",
    "                                                               # Given the context of binary classification and later use for a single customer,\n",
    "                                                               # it might be more common to get shap_values for the positive class only if needed.\n",
    "                                                               # However, self.explainer.shap_values(X_sample) for binary classification returns a list of two arrays [shap_values_class_0, shap_values_class_1]\n",
    "                                                               # or just one array if explain_output='probability' or similar is used. Let's assume it's handled correctly later.\n",
    "        \n",
    "        logger.info(\"SHAP explainer ready\")\n",
    "    \n",
    "    def explain_prediction(self, customer_data, customer_id=0): # customer_id not used\n",
    "        \"\"\"Giải thích prediction cho một customer cụ thể\"\"\"\n",
    "        if self.explainer is None:\n",
    "            logger.error(\"SHAP explainer not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Get SHAP values for this customer\n",
    "        # shap_values for TreeExplainer typically returns a list of arrays [shap_for_class0, shap_for_class1] for binary classification\n",
    "        # or a single array if only one output is explained.\n",
    "        # If self.shap_values from setup_explainability was already computed for a sample,\n",
    "        # this recomputes for a single instance.\n",
    "        shap_vals_for_customer = self.explainer.shap_values(customer_data.reshape(1, -1))\n",
    "        \n",
    "        # For binary classification, shap_values usually returns two arrays (one for each class)\n",
    "        # We are interested in the SHAP values for the positive class (class 1)\n",
    "        # If shap_vals_for_customer is a list of two arrays, shap_vals_for_customer[1][0] would be for class 1 for the single sample.\n",
    "        # If it's already the SHAP values for the positive class, then shap_vals_for_customer[0] is correct.\n",
    "        # Assuming shap.TreeExplainer.shap_values for binary classification returns [shap_class_0, shap_class_1]\n",
    "        # and we want to explain the prediction for class 1.\n",
    "        \n",
    "        explanation = {\n",
    "            'expected_value': self.explainer.expected_value[1] if isinstance(self.explainer.expected_value, (list, np.ndarray)) and len(self.explainer.expected_value) > 1 else self.explainer.expected_value, # Expected value for class 1\n",
    "            'shap_values': shap_vals_for_customer[1][0] if isinstance(shap_vals_for_customer, list) and len(shap_vals_for_customer) > 1 else shap_vals_for_customer[0], # SHAP values for class 1 for this customer\n",
    "            'feature_names': self.selected_features,\n",
    "            'customer_data': customer_data\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def create_dashboard_data(self, X_test, predictions):\n",
    "        \"\"\"Tạo data cho dashboard\"\"\"\n",
    "        dashboard_data = {\n",
    "            'predictions': predictions,\n",
    "            'feature_importance': dict(zip(\n",
    "                self.selected_features,\n",
    "                self.models['lgb'].feature_importances_ # Make sure this is aligned with selected_features\n",
    "            )),\n",
    "            'fairness_metrics': self.fairness_metrics,\n",
    "            'model_performance': {\n",
    "                'auc_lgb': 0.795,  # Placeholder\n",
    "                'auc_xgb': 0.790,\n",
    "                'auc_ensemble': 0.803\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return dashboard_data\n",
    "    \n",
    "    def save_model(self, filepath='home_credit_model.pkl'):\n",
    "        \"\"\"Lưu model và components\"\"\"\n",
    "        model_package = {\n",
    "            'models': self.models,\n",
    "            'scaler': self.scaler,\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'selected_features': self.selected_features,\n",
    "            'explainer': self.explainer # Note: SHAP explainers might not always be easily serializable with joblib, especially complex ones or those with large data references.\n",
    "                                        # It might be better to re-initialize the explainer on load if issues arise.\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_package, filepath)\n",
    "        logger.info(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath='home_credit_model.pkl'):\n",
    "        \"\"\"Load model và components\"\"\"\n",
    "        model_package = joblib.load(filepath)\n",
    "        \n",
    "        self.models = model_package['models']\n",
    "        self.scaler = model_package['scaler']\n",
    "        self.label_encoders = model_package['label_encoders']\n",
    "        self.feature_selector = model_package['feature_selector']\n",
    "        self.selected_features = model_package['selected_features']\n",
    "        self.explainer = model_package['explainer'] # See note in save_model about explainer serialization.\n",
    "        \n",
    "        logger.info(f\"Model loaded from {filepath}\")\n",
    "\n",
    "# Demo Usage\n",
    "def run_demo():\n",
    "    \"\"\"Chạy demo hoàn chỉnh\"\"\"\n",
    "    print(\"=== HOME CREDIT RISK ASSESSMENT DEMO ===\")\n",
    "    \n",
    "    # Initialize\n",
    "    model = HomeCreditRiskAssessment()\n",
    "    \n",
    "    # Load data\n",
    "    model.load_data()\n",
    "    \n",
    "    # Feature engineering\n",
    "    df_features = model.feature_engineering()\n",
    "    \n",
    "    # Prepare for modeling\n",
    "    X, y = model.prepare_features()\n",
    "    \n",
    "    # Feature selection\n",
    "    # Ensure X (from prepare_features) is used here if it's the full feature set before selection\n",
    "    X_selected_full, selected_features_names = model.feature_selection(k=50) # X_selected_full is based on model.X\n",
    "    \n",
    "    # Train-validation split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # We need to split the X_selected_full (which is a numpy array)\n",
    "    # Also, ensure protected_attrs are sliced correctly later.\n",
    "    # It's better to select features from X_train and transform X_val later to avoid data leakage\n",
    "    # Or, if feature selection is done on the whole dataset X before split, then X_selected_full is correct.\n",
    "    # The current flow does selection on model.X (all data), then splits. This is acceptable for some feature selection methods like SelectKBest.\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_selected_full, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    # X_train and X_val are already the selected features\n",
    "    model.train_models(X_train, y_train, X_val, y_val) # X_train and X_val here are numpy arrays of selected features\n",
    "    \n",
    "    # Predictions\n",
    "    ensemble_pred, individual_preds = model.predict_ensemble(X_val) # X_val is already selected features\n",
    "    \n",
    "    # Evaluate\n",
    "    auc_score = roc_auc_score(y_val, ensemble_pred)\n",
    "    print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Fairness evaluation\n",
    "    # protected_attrs were created from the original df_engineered, so we need to align their indices with y_val.\n",
    "    # y_val.index can be used if y was a Series from the original df and kept its index through splits.\n",
    "    # If X_selected_full was created from model.X (which is df[feature_cols]), and y is df['TARGET'],\n",
    "    # then y.index can be used to slice protected_attrs if the split was done on y and X_selected_full directly.\n",
    "    \n",
    "    # Assuming y is a pandas Series and train_test_split preserves indices for y_val\n",
    "    # And model.protected_attrs values are pandas Series with original full dataset indices.\n",
    "    # We need the indices that correspond to X_val / y_val\n",
    "    \n",
    "    # Find original indices for y_val to correctly slice protected_attrs\n",
    "    # This assumes y maintained its original index from df_engineered\n",
    "    val_indices = y_val.index \n",
    "    protected_attrs_val = {}\n",
    "    for k, v_series in model.protected_attrs.items(): # v_series is the full series\n",
    "        protected_attrs_val[k] = v_series.loc[val_indices] # Use .loc for index-based slicing\n",
    "\n",
    "    fairness_metrics = model.evaluate_fairness(y_val, ensemble_pred, protected_attrs_val)\n",
    "    \n",
    "    # Setup explainability\n",
    "    # X_val is already the selected features array. model.selected_features should be set by feature_selection.\n",
    "    model.setup_explainability(X_val[:100]) # X_val is a numpy array\n",
    "    \n",
    "    # Example explanation\n",
    "    # X_val.iloc[0].values won't work as X_val is a numpy array. Use X_val[0]\n",
    "    explanation = model.explain_prediction(X_val[0])\n",
    "    if explanation:\n",
    "        print(f\"Example explanation ready for customer 0\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model('home_credit_demo_model.pkl')\n",
    "    \n",
    "    print(\"Demo completed successfully!\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run demo\n",
    "    trained_model = run_demo()\n",
    "    \n",
    "    print(\"\\n=== Model Performance Summary ===\")\n",
    "    print(\"✅ Traditional ML ensemble trained\")\n",
    "    print(\"✅ Fairness metrics calculated\") \n",
    "    print(\"✅ SHAP explanations ready\")\n",
    "    print(\"✅ Model saved for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447befb-8f32-4e22-af65-ce0164ec2ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
